{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#database view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "counties\n",
      "climate\n",
      "biodiversity\n",
      "air_quality\n",
      "pollutant_emissions\n",
      "toxic_release_inventory\n",
      "\n",
      "First five rows of the table 'toxic_release_inventory':\n",
      "('82716NLSMP13151', 'Copper', Decimal('44.28536600'), Decimal('-105.38249000'), 2022, 'N', Decimal('2.00'), 'N', 'N', 'N', 56005, 'to air')\n",
      "('82716NLSMP13151', 'Manganese', Decimal('44.28536600'), Decimal('-105.38249000'), 2022, 'N', Decimal('2.00'), 'N', 'N', 'N', 56005, 'to air')\n",
      "('82716NLSMP13151', 'Ammonia', Decimal('44.28536600'), Decimal('-105.38249000'), 2022, 'N', Decimal('31201.00'), 'N', 'N', 'N', 56005, 'to air')\n",
      "('82716NLSMP13151', 'Vanadium (except when contained in an alloy)', Decimal('44.28536600'), Decimal('-105.38249000'), 2022, 'N', Decimal('61.00'), 'N', 'N', 'N', 56005, 'to air')\n",
      "('82716NLSMP13151', 'Nickel', Decimal('44.28536600'), Decimal('-105.38249000'), 2022, 'N', Decimal('0.10'), 'Y', 'N', 'N', 56005, 'to air')\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import psycopg2.extras\n",
    "import pandas as pd\n",
    "\n",
    "db_params = {\n",
    "    'dbname': 'postgres',\n",
    "    'user': 'postgres',\n",
    "    'password': '123456',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Check if the table exists\n",
    "            cursor.execute(\"SELECT tablename FROM pg_catalog.pg_tables WHERE schemaname != 'pg_catalog' AND schemaname != 'information_schema';\")\n",
    "            tables = cursor.fetchall()\n",
    "            print(\"Tables in the database:\")\n",
    "            for table in tables:\n",
    "                print(table[0])\n",
    "\n",
    "            table_name = 'toxic_release_inventory'\n",
    "            cursor.execute(f\"SELECT * FROM {table_name} LIMIT 5\")\n",
    "            rows = cursor.fetchall()\n",
    "            print(f\"\\nFirst five rows of the table '{table_name}':\")\n",
    "            for row in rows:\n",
    "                print(row)\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the 'climate' table:\n",
      "climate_id\n",
      "year\n",
      "month\n",
      "day\n",
      "value\n",
      "county_code\n",
      "facility_id\n",
      "sflag\n",
      "mflag\n",
      "qflag\n",
      "element\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT column_name \n",
    "                FROM information_schema.columns \n",
    "                WHERE table_name = 'climate'\n",
    "            \"\"\")\n",
    "            columns = cursor.fetchall()\n",
    "            print(\"Columns in the 'climate' table:\")\n",
    "            for column in columns:\n",
    "                print(column[0])\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#county insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../final_data/county_info.csv')\n",
    "\n",
    "df = df[['geoid', 'county', 'latitude', 'longitude', 'state']]\n",
    "df.columns = ['county_code', 'county_name', 'latitude', 'longitude', 'state_code']\n",
    "\n",
    "# Convert state_code to a string with 2 characters\n",
    "df['state_code'] = df['state_code'].apply(lambda x: '{:02}'.format(x) if isinstance(x, int) else x)\n",
    "\n",
    "df['county_code'] = df['county_code'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            insert_query = \"\"\"\n",
    "            INSERT INTO counties (county_code, county_name, latitude, longitude, state_code)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (county_code) DO NOTHING;\n",
    "            \"\"\"\n",
    "            for row in df.itertuples(index=False, name=None):\n",
    "                cursor.execute(insert_query, row)\n",
    "            conn.commit()\n",
    "    print(\"Data inserted successfully\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#biodiversity table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in DataFrame: 32\n",
      "Number of placeholders in SQL query: 32\n",
      "Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "biodiversity_df = pd.read_csv('../final_data/biodb_with_county.csv')\n",
    "\n",
    "biodiversity_df['geoid'] = biodiversity_df['geoid'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "biodiversity_df = biodiversity_df.drop(columns=['county','state'])\n",
    "\n",
    "biodiversity_df.fillna(0, inplace=True)\n",
    "\n",
    "biodiversity_df = biodiversity_df.drop_duplicates(subset='gbifID', keep='first')\n",
    "\n",
    "biodiversity_df = biodiversity_df[biodiversity_df['geoid'] != '00000']\n",
    "\n",
    "biodiversity_df.rename(columns={\n",
    "    'gbifID': 'gbif_id',\n",
    "    'datasetKey': 'dataset_key',\n",
    "    'occurrenceID': 'occurrence_id',\n",
    "    'order': 'orders',\n",
    "    'infraspecific_epithet': 'infraspecificEpithet',\n",
    "    'taxon_rank': 'taxonRank',\n",
    "    'scientific_name': 'scientificName',\n",
    "    'verbatim_scientific_name': 'verbatimScientificName',\n",
    "    'occurrence_status': 'occurrenceStatus',\n",
    "    'individual_count': 'individualCount',\n",
    "    'publishing_org_key': 'publishingOrgKey',\n",
    "    'decimal_latitude': 'decimalLatitude',\n",
    "    'decimal_longitude': 'decimalLongitude',\n",
    "    'coordinate_uncertainty_in_meters': 'coordinateUncertaintyInMeters',\n",
    "    'event_date': 'eventDate',\n",
    "    'taxon_key': 'taxonKey',\n",
    "    'species_key': 'speciesKey',\n",
    "    'basis_of_record': 'basisOfRecord',\n",
    "    'collection_code': 'collectionCode',\n",
    "    'catalog_number': 'catalogNumber',\n",
    "    'last_interpreted': 'lastInterpreted',\n",
    "    'media_type': 'mediaType',\n",
    "    'geoid': 'county_code'\n",
    "}, inplace=True)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO biodiversity (\n",
    "    gbif_id, dataset_key, occurrence_id, kingdom, phylum, class, orders, family, genus, species,\n",
    "    infraspecific_epithet, taxon_rank, scientific_name, verbatim_scientific_name, locality, occurrence_status,\n",
    "    individual_count, publishing_org_key, decimal_latitude, decimal_longitude, coordinate_uncertainty_in_meters,\n",
    "    event_date, taxon_key, species_key, basis_of_record, collection_code, catalog_number, license,\n",
    "    last_interpreted, media_type, issue, county_code\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(biodiversity_df.columns)}\")\n",
    "placeholder_count = insert_query.count('%s')\n",
    "print(f\"Number of placeholders in SQL query: {placeholder_count}\")\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            for row in biodiversity_df.itertuples(index=False, name=None):\n",
    "                cursor.execute(insert_query, row)\n",
    "            conn.commit()\n",
    "    print(\"Data inserted successfully\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#climate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv('../final_data/final_weather_data.csv', low_memory=False)\n",
    "\n",
    "# Ensure geoid is 5 digits\n",
    "climate_df['geoid'] = climate_df['geoid'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "climate_df = climate_df.drop(columns=['county', 'state'])\n",
    "\n",
    "climate_df = climate_df[climate_df['geoid'] != '00000']\n",
    "\n",
    "# Data cleaning: Ensure numeric columns contain only numeric values or NaN\n",
    "numeric_columns = [f\"VALUE{i}\" for i in range(1, 32)] \n",
    "for col in numeric_columns:\n",
    "    climate_df[col] = pd.to_numeric(climate_df[col], errors='coerce')  # Convert non-numeric to NaN\n",
    "\n",
    "normalized_data = []\n",
    "\n",
    "# Iterate over the DataFrame and create a new record for each day\n",
    "for index, row in climate_df.iterrows():\n",
    "    for day in range(1, 32): \n",
    "        mflag = str(row.get(f'MFLAG{day}', ''))[0] if pd.notnull(row.get(f'MFLAG{day}')) else None\n",
    "        qflag = str(row.get(f'QFLAG{day}', ''))[0] if pd.notnull(row.get(f'QFLAG{day}')) else None\n",
    "        sflag = str(row.get(f'SFLAG{day}', ''))[0] if pd.notnull(row.get(f'SFLAG{day}')) else None\n",
    "        record = {\n",
    "            'facility_id': row['ID'],\n",
    "            'year': row['YEAR'],\n",
    "            'month': row['MONTH'],\n",
    "            'day': day,\n",
    "            'element': row['ELEMENT'],\n",
    "            'value': row.get(f'VALUE{day}', None),\n",
    "            'mflag': mflag,\n",
    "            'qflag': qflag,\n",
    "            'sflag': sflag,\n",
    "            'county_code': row['geoid']\n",
    "        }\n",
    "        normalized_data.append(record)\n",
    "\n",
    "data_tuples = [tuple(record.values()) for record in normalized_data]\n",
    "\n",
    "conn = None\n",
    "try:\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    with conn.cursor() as cursor:\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO climate (facility_id, year, month, day, element,\n",
    "                             value, mflag, qflag, sflag, county_code)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        for data_tuple in data_tuples:\n",
    "            cursor.execute(insert_query, data_tuple)\n",
    "        conn.commit() \n",
    "    print(\"Data inserted successfully\")\n",
    "except Error as e: \n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#air_quality table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in DataFrame: 20\n",
      "Number of placeholders in SQL query: 20\n",
      "Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "ozone_df = pd.read_csv('../final_data/Ozone_Levels_Daily_Records_2022.csv')\n",
    "carbon_mono_df = pd.read_csv('../final_data/Carbon_Monoxide_Levels_Daily_Records_2022.csv')\n",
    "\n",
    "air_quality_df = pd.concat([ozone_df, carbon_mono_df])\n",
    "\n",
    "air_quality_df['geoid'] = air_quality_df['geoid'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "air_quality_df = air_quality_df.drop(columns=['county', 'state'])\n",
    "\n",
    "numeric_cols = ['Observation Count', 'Observation Percent', 'Arithmetic Mean', '1st Max Value', '1st Max Hour', 'AQI']\n",
    "air_quality_df[numeric_cols] = air_quality_df[numeric_cols].fillna(0)\n",
    "\n",
    "air_quality_df = air_quality_df[air_quality_df['geoid'] != '00000']\n",
    "\n",
    "air_quality_df.rename(columns={\n",
    "    'Site Num': 'site_num',\n",
    "    'Latitude': 'latitude',\n",
    "    'Longitude': 'longitude',\n",
    "    'Datum':'datum',\n",
    "    'Parameter Name':'parameter_name',\n",
    "    'Sample Duration':'sample_duration',\n",
    "    'Pollutant Standard':'pollutant_standard',\n",
    "    'Date Local':'date_local',\n",
    "    'Units of Measure':'units_of_measure',\n",
    "    'Event Type,Observation Count':'event_type',\n",
    "    'Observation Count':'observation_count',\n",
    "    'Observation Percent':'observation_percent',\n",
    "    'Arithmetic Mean':'arithmetic_mean',\n",
    "    '1st Max Value':'max_value',\n",
    "    '1st Max Hour':'max_hour',\n",
    "    'AQI':'aqi',\n",
    "    'Local Site Name':'local_site_name',\n",
    "    'Address':'address',\n",
    "    'City Name':'city_name'\n",
    "}, inplace=True)\n",
    "\n",
    "air_quality_df.drop_duplicates(subset=['site_num', 'date_local', 'parameter_name'], keep='first', inplace=True)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO air_quality (\n",
    "    site_num, latitude, longitude, datum, parameter_name,\n",
    "    sample_duration, pollutant_standard, date_local, units_of_measure,\n",
    "    event_type, observation_count, observation_percent, arithmetic_mean,\n",
    "    max_value, max_hour, aqi, local_site_name, address, city_name,\n",
    "    geoid\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(air_quality_df.columns)}\")\n",
    "\n",
    "placeholder_count = insert_query.count('%s')\n",
    "print(f\"Number of placeholders in SQL query: {placeholder_count}\")\n",
    "\n",
    "assert len(air_quality_df.columns) == placeholder_count, \"The number of placeholders does not match the number of DataFrame columns.\"\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            for row in air_quality_df.itertuples(index=False, name=None):\n",
    "                cursor.execute(insert_query, row)\n",
    "            conn.commit()\n",
    "    print(\"Data inserted successfully\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pollutant_emissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in DataFrame: 9\n",
      "Number of placeholders in SQL query: 9\n",
      "Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "pollutant_emissions_df = pd.read_csv('../final_data/nei_faculty_sample10000.csv')\n",
    "\n",
    "pollutant_emissions_df['geoid'] = pollutant_emissions_df['geoid'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "pollutant_emissions_df = pollutant_emissions_df[pollutant_emissions_df['geoid'] != '00000']\n",
    "\n",
    "pollutant_emissions_df.rename(columns={\n",
    "    'SITE  NAME': 'site_name',\n",
    "    'Latitude': 'latitude',\n",
    "    'Longitude': 'longitude',\n",
    "    'Pollutant': 'pollutant',\n",
    "    'Pollutant Type': 'pollutant_type',\n",
    "    'Facility Type': 'facility_type',\n",
    "    'NAICS': 'naics',\n",
    "    'Emissions (Tons)': 'emissions_tons', \n",
    "    'geoid': 'county_code'\n",
    "}, inplace=True)\n",
    "\n",
    "pollutant_emissions_df.drop_duplicates(subset=['site_name', 'pollutant'], keep='first', inplace=True)\n",
    "\n",
    "pollutant_emissions_df = pollutant_emissions_df[[\n",
    "    'latitude', 'longitude', 'site_name', 'pollutant', 'pollutant_type',\n",
    "    'facility_type', 'naics', 'emissions_tons', 'county_code'\n",
    "]]\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO pollutant_emissions (\n",
    "    latitude, longitude, site_name, pollutant, pollutant_type,\n",
    "    facility_type, naics, emissions_tons, county_code\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(pollutant_emissions_df.columns)}\")\n",
    "\n",
    "placeholder_count = insert_query.count('%s')\n",
    "print(f\"Number of placeholders in SQL query: {placeholder_count}\")\n",
    "\n",
    "assert len(pollutant_emissions_df.columns) == placeholder_count, \"The number of placeholders does not match the number of DataFrame columns.\"\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            for row in pollutant_emissions_df.itertuples(index=False, name=None):\n",
    "                cursor.execute(insert_query, row)\n",
    "            conn.commit()\n",
    "    print(\"Data inserted successfully\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pollutant_emissions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in DataFrame: 12\n",
      "Number of placeholders in SQL query: 12\n",
      "Data inserted successfully\n"
     ]
    }
   ],
   "source": [
    "toxic_release_inventory_df = pd.read_csv('../final_data/toxic_release_inventory.csv').rename(str.lower, axis='columns')\n",
    "\n",
    "toxic_release_inventory_df = toxic_release_inventory_df[[\n",
    "    'tri_facility_id', 'latitude', 'longitude', \n",
    "    'reporting_year', 'chem_name', 'elemental_metal_included', \n",
    "    'rel_est_amt', 'carcinogen', 'pbt_ind', 'pfas_ind', \n",
    "    'discharge_to', 'geoid'\n",
    "]]\n",
    "\n",
    "toxic_release_inventory_df.rename(columns={\n",
    "    'geoid': 'county_code'\n",
    "}, inplace=True)\n",
    "\n",
    "toxic_release_inventory_df['county_code'] = toxic_release_inventory_df['county_code'].astype(str).str.zfill(5)\n",
    "toxic_release_inventory_df = toxic_release_inventory_df[toxic_release_inventory_df['county_code'] != '00000']\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO toxic_release_inventory (\n",
    "    tri_facility_id, latitude, longitude, \n",
    "    reporting_year, chem_name, elemental_metal_included, rel_est_amt, \n",
    "    carcinogen, pbt_ind, pfas_ind, \n",
    "    discharge_to, county_code\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "ON CONFLICT (tri_facility_id, chem_name, reporting_year) DO NOTHING;\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Number of columns in DataFrame: {len(toxic_release_inventory_df.columns)}\")\n",
    "placeholder_count = insert_query.count('%s')\n",
    "print(f\"Number of placeholders in SQL query: {placeholder_count}\")\n",
    "\n",
    "assert len(toxic_release_inventory_df.columns) == insert_query.count('%s'), \"The number of placeholders does not match the number of DataFrame columns.\"\n",
    "\n",
    "try:\n",
    "    with psycopg2.connect(**db_params) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            for row in toxic_release_inventory_df.itertuples(index=False, name=None):\n",
    "                cursor.execute(insert_query, row)\n",
    "            conn.commit()\n",
    "    print(\"Data inserted successfully\")\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if conn is not None:\n",
    "        conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
